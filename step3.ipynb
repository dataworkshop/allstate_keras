{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vova/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = train.sample(n=100000)\n",
    "\n",
    "y = np.log( df_train['loss'].values )\n",
    "sparse_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_cats = [f for f in df_train.columns if 'cat' in f]\n",
    "for feat in feat_cats:\n",
    "    dummy = pd.get_dummies(df_train[feat].astype('category'))\n",
    "    tmp = csr_matrix(dummy)\n",
    "    sparse_data.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_num = [f for f in df_train.columns if 'cont' in f]\n",
    "scaler = StandardScaler()\n",
    "tmp = csr_matrix(scaler.fit_transform(df_train[f_num]))\n",
    "sparse_data.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100000x1093 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 13000000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = hstack(sparse_data, format = 'csr')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c7e27479fb42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(400, input_dim = input_dim, init = 'he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.45))\n",
    "\n",
    "    model.add(Dense(200, init = 'he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(50, init = 'he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, init = 'he_normal'))\n",
    "    model.compile(loss = 'mae', optimizer = 'adadelta')\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "        \n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "            \n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "14s - loss: 4.0804 - val_loss: 0.7600\n",
      "Epoch 2/4\n",
      "21s - loss: 1.0442 - val_loss: 0.4936\n",
      "Epoch 3/4\n",
      "39s - loss: 0.8669 - val_loss: 0.4824\n",
      "Epoch 4/4\n",
      "41s - loss: 0.7869 - val_loss: 0.4675\n",
      "Fold1, score=1290.45410169\n",
      "Epoch 1/4\n",
      "25s - loss: 4.2003 - val_loss: 0.6841\n",
      "Epoch 2/4\n",
      "15s - loss: 1.1220 - val_loss: 0.5431\n",
      "Epoch 3/4\n",
      "16s - loss: 0.9196 - val_loss: 0.4622\n",
      "Epoch 4/4\n",
      "16s - loss: 0.8166 - val_loss: 0.4566\n",
      "Fold2, score=1249.59718595\n",
      "Epoch 1/4\n",
      "15s - loss: 4.2140 - val_loss: 0.7820\n",
      "Epoch 2/4\n",
      "17s - loss: 1.0618 - val_loss: 0.4949\n",
      "Epoch 3/4\n",
      "17s - loss: 0.8871 - val_loss: 0.4652\n",
      "Epoch 4/4\n",
      "16s - loss: 0.8004 - val_loss: 0.4635\n",
      "Fold3, score=1285.78028202\n"
     ]
    }
   ],
   "source": [
    "nepochs = 4\n",
    "nfolds = 3\n",
    "folds = KFold(len(y), n_folds=nfolds, shuffle = True, random_state = 2017)\n",
    "\n",
    "\n",
    "\n",
    "for num_iter, (train_index, test_index) in enumerate(folds):\n",
    "    X_train, y_train = X[train_index], y[train_index]\n",
    "    X_test, y_test   = X[test_index], y[test_index]\n",
    "        \n",
    "    model = nn_model(X_train.shape[1])\n",
    "    callbacks=[EarlyStopping(patience=8)]\n",
    "\n",
    "    model.fit_generator(generator = batch_generator(X_train, y_train, 128, True),\n",
    "                                  nb_epoch = nepochs,\n",
    "                                  samples_per_epoch = y_train.shape[0],\n",
    "                                  validation_data=(X_test.todense(), y_test),\n",
    "                                  verbose = 2, callbacks=callbacks) \n",
    "    \n",
    "    y_pred = np.exp(model.predict_generator(generator = batch_generatorp(X_test, 128, False), val_samples = X_test.shape[0])[:,0])\n",
    "\n",
    "    score = mean_absolute_error(np.exp(y_test), y_pred)\n",
    "    print(\"Fold{0}, score={1}\".format(num_iter+1, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task\n",
    "\n",
    "Play aroud with NN architecture. First version is here:\n",
    "\n",
    "- input\n",
    "- hidden1: 400\n",
    "- drouput + bn\n",
    "- hidden2: 200\n",
    "- drouput + bn\n",
    "- hidden3: 50\n",
    "- output\n",
    "\n",
    "\n",
    "try to change something (remove layer, add a new one, change attribute in dropout and so on)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
